{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Go until jurong point, crazy.. Available only ...\n",
      "1                        Ok lar... Joking wif u oni...\n",
      "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3    U dun say so early hor... U c already then say...\n",
      "4    Nah I don't think he goes to usf, he lives aro...\n",
      "Name: text, dtype: object\n",
      "0     ham\n",
      "1     ham\n",
      "2    spam\n",
      "3     ham\n",
      "4     ham\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('spam.csv',encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "#print(data.head())\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "print(texts[0:5])\n",
    "print(tags[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10081\n",
      "[('call', 392), ('get', 335), ('Im', 313), ('ur', 293), ('ltgt', 276), ('You', 267), ('know', 251), ('go', 247), ('like', 230), ('dont', 217), ('come', 207), ('got', 206), ('time', 191), ('day', 180), ('No', 173), ('want', 167), ('Ill', 165), ('lor', 160), ('Call', 158), ('home', 156), ('send', 153), ('going', 152), ('one', 152), ('need', 150), ('Ok', 147), ('good', 145), ('love', 143), ('How', 143), ('back', 141), ('still', 137), ('text', 135), ('But', 133), ('If', 133), ('im', 129), ('later', 127), ('see', 124), ('da', 121), ('ok', 119), ('So', 119), ('Just', 119), ('We', 119), ('think', 118), ('Its', 117), ('free', 116), ('FREE', 113), ('Do', 113), ('today', 112), ('Sorry', 112), ('week', 111), ('phone', 111)]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_sms(vocab):\n",
    "    for t in texts:\n",
    "        tokens = clean_txt(t)\n",
    "        vocab.update(tokens)\n",
    "\n",
    "def clean_txt(txt):\n",
    "    # split into tokens \n",
    "    txt = str(txt)\n",
    "    tokens = txt.split()\n",
    "    # remove punctuation \n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove  not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter  stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "vocab = Counter()\n",
    "process_sms(vocab)\n",
    "print(len(vocab))\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4517\n"
     ]
    }
   ],
   "source": [
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))\n",
    "vocab = set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_embedding(filename):\n",
    "    file = open(filename,'r',encoding=\"utf-8\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sms():\n",
    "    documents = list()\n",
    "    for text in texts:\n",
    "        tokens = clean_text(text, vocab)\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "def clean_text(text, vocab):\n",
    "    # split into tokens \n",
    "    tokens = str(text).split()\n",
    "    # remove punctuation \n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "vocab = set(tokens)\n",
    "train_texts = process_sms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import merge, Dense, LSTM, Dropout\n",
    "from keras.layers import Flatten, Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "# sequence encode\n",
    "encoded_texts = tokenizer.texts_to_sequences(train_texts)\n",
    "# pad sequences\n",
    "\n",
    "max_length = max([len(s.split()) for s in train_texts])\n",
    "All_sms = pad_sequences(encoded_texts, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_labels = []\n",
    "for t in tags:\n",
    "    if t == 'ham':\n",
    "        All_labels.append(0)\n",
    "    elif t == 'spam':\n",
    "        All_labels.append(1)\n",
    "        \n",
    "\n",
    "#Splitting test and train sets\n",
    "Xtrain = All_sms[:4180] #75% of all smses\n",
    "Xtest = All_sms[4180:]\n",
    "\n",
    "ytrain = All_labels[:4180]\n",
    "ytest = All_labels[4180:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('glove.6B.100d.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 77, 100)           357400    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 73, 128)           64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 36, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4609      \n",
      "=================================================================\n",
      "Total params: 426,137\n",
      "Trainable params: 68,737\n",
      "Non-trainable params: 357,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\sattar\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.2150 - accuracy: 0.9285\n",
      "Epoch 2/10\n",
      " - 4s - loss: 0.0896 - accuracy: 0.9725\n",
      "Epoch 3/10\n",
      " - 4s - loss: 0.0590 - accuracy: 0.9823\n",
      "Epoch 4/10\n",
      " - 4s - loss: 0.0380 - accuracy: 0.9897\n",
      "Epoch 5/10\n",
      " - 4s - loss: 0.0280 - accuracy: 0.9923\n",
      "Epoch 6/10\n",
      " - 4s - loss: 0.0174 - accuracy: 0.9964\n",
      "Epoch 7/10\n",
      " - 4s - loss: 0.0126 - accuracy: 0.9969\n",
      "Epoch 8/10\n",
      " - 4s - loss: 0.0094 - accuracy: 0.9983\n",
      "Epoch 9/10\n",
      " - 4s - loss: 0.0075 - accuracy: 0.9988\n",
      "Epoch 10/10\n",
      " - 4s - loss: 0.0064 - accuracy: 0.9988\n",
      "Test Accuracy: 97.054595\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2926 samples, validate on 1254 samples\n",
      "Epoch 1/10\n",
      "2926/2926 [==============================] - 6s 2ms/step - loss: 0.3212 - accuracy: 0.8882 - val_loss: 0.1597 - val_accuracy: 0.9506\n",
      "Epoch 2/10\n",
      "2926/2926 [==============================] - 6s 2ms/step - loss: 0.1549 - accuracy: 0.9487 - val_loss: 0.1246 - val_accuracy: 0.9593\n",
      "Epoch 3/10\n",
      "2926/2926 [==============================] - 6s 2ms/step - loss: 0.1157 - accuracy: 0.9648 - val_loss: 0.1177 - val_accuracy: 0.9625\n",
      "Epoch 4/10\n",
      "2926/2926 [==============================] - 6s 2ms/step - loss: 0.0948 - accuracy: 0.9720 - val_loss: 0.1131 - val_accuracy: 0.9649\n",
      "Epoch 5/10\n",
      "2926/2926 [==============================] - 6s 2ms/step - loss: 0.0859 - accuracy: 0.9754 - val_loss: 0.1159 - val_accuracy: 0.9657\n",
      "Epoch 6/10\n",
      "2926/2926 [==============================] - 6s 2ms/step - loss: 0.0669 - accuracy: 0.9792 - val_loss: 0.1151 - val_accuracy: 0.9657\n",
      "Epoch 7/10\n",
      "2926/2926 [==============================] - 6s 2ms/step - loss: 0.0591 - accuracy: 0.9843 - val_loss: 0.1137 - val_accuracy: 0.9721\n",
      "Epoch 8/10\n",
      "2926/2926 [==============================] - 6s 2ms/step - loss: 0.0572 - accuracy: 0.9822 - val_loss: 0.1254 - val_accuracy: 0.9689\n",
      "Epoch 9/10\n",
      "2926/2926 [==============================] - 6s 2ms/step - loss: 0.0385 - accuracy: 0.9915 - val_loss: 0.1058 - val_accuracy: 0.9713\n",
      "Epoch 10/10\n",
      "2926/2926 [==============================] - 6s 2ms/step - loss: 0.0314 - accuracy: 0.9921 - val_loss: 0.1393 - val_accuracy: 0.9713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a30258c908>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv_LSTM = Sequential()\n",
    "model_conv_LSTM.add(embedding_layer)\n",
    "model_conv_LSTM.add(Dropout(0.2))\n",
    "model_conv_LSTM.add(Conv1D(64, 5, activation='relu'))\n",
    "model_conv_LSTM.add(MaxPooling1D(pool_size=4))\n",
    "model_conv_LSTM.add(LSTM(100))\n",
    "model_conv_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "model_conv_LSTM.compile(loss='binary_crossentropy', optimizer='adam',    metrics=['accuracy'])\n",
    "model_conv_LSTM.fit(Xtrain, ytrain, validation_split=0.3, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.198278\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model_conv_LSTM.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 77, 100)           357400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 77, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 73, 128)           64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 18, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 15, 64)            32832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 6, 32)             4128      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 3, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 458,585\n",
      "Trainable params: 101,185\n",
      "Non-trainable params: 357,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.2303 - accuracy: 0.9187\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.1114 - accuracy: 0.9629\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.0787 - accuracy: 0.9737\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.0744 - accuracy: 0.9739\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.0603 - accuracy: 0.9789\n",
      "Epoch 6/10\n",
      " - 5s - loss: 0.0385 - accuracy: 0.9878\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.0254 - accuracy: 0.9923\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.0264 - accuracy: 0.9919\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.0189 - accuracy: 0.9943\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.0273 - accuracy: 0.9909\n",
      "Test Accuracy: 97.557473\n"
     ]
    }
   ],
   "source": [
    "# define 2 CNN Layer model\n",
    "model_2CNN = Sequential()\n",
    "model_2CNN.add(embedding_layer)\n",
    "model_2CNN.add(Dropout(0.3))\n",
    "model_2CNN.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model_2CNN.add(MaxPooling1D(pool_size=4))\n",
    "model_2CNN.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n",
    "model_2CNN.add(MaxPooling1D(pool_size=2))\n",
    "model_2CNN.add(Conv1D(filters=32, kernel_size=2, activation='relu'))\n",
    "model_2CNN.add(MaxPooling1D(pool_size=2))\n",
    "model_2CNN.add(Flatten())\n",
    "model_2CNN.add(Dense(1, activation='sigmoid'))\n",
    "print(model_2CNN.summary())\n",
    "# compile network\n",
    "model_2CNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_2CNN.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model_2CNN.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
